{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='top'></a>    \n",
    "<div style=\"width:900px;background:#fdf0db;border:1px solid black;text-align:left;padding:8px;\">\n",
    "    <p>\n",
    "        <span style=\"font-size:14pt\">\n",
    "            <b>Scraping from www.mycareersfuture.sg (Detail Page)</b>\n",
    "        </span>\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "<div style=\"width:900px;background:#f2f2f2;border:1px solid black;text-align:left;padding:8px;\">\n",
    "\n",
    "This notebook file will extract detail information from Job Details extracted from the 1st Notebook file,(01-webscrape-pages.ipynb).\n",
    "\n",
    "The Details such as Job Description and Job Requirements will be extracted to further help extract more details for analysis.  \n",
    "\n",
    "\n",
    "In order to run this project please execute the following:\n",
    "1. Run 01-webscrape-pages.ipynb\n",
    "2. Run 02-webscrape-details.ipynb\n",
    "3. Move all output files created by these two notebooks to data folder.\n",
    "4. Run 03-data-val-and-cleaning.ipynb\n",
    "   - This notebook will create a final csv file in data folder.\n",
    "5. Run 04-data-modelling.ipynb\n",
    "   - This notebook will read the final csv file to perform analysis.\n",
    "   \n",
    "Please note that All output files produced by the first two notebooks (01-webscrape-pages.ipynb & 02-webscrape-details.ipynb) will be written to the current directory.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from selenium import webdriver\n",
    "# Import sleep:\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['outfile_00', 'outfile_01', 'outfile_02', 'outfile_03', 'outfile_04', 'outfile_05']\n",
      "['outfile_a00', 'outfile_a01', 'outfile_a02', 'outfile_a03', 'outfile_a04']\n"
     ]
    }
   ],
   "source": [
    "## Generate a list of page files to be processed (remove the .csv extension)\n",
    "myfiles=['outfile_0{}'.format(x) for x in range(6)]  \n",
    "\n",
    "for x in range(5):\n",
    "    myfiles.append('outfile_a0{}'.format(x))\n",
    "    \n",
    "print(myfiles[:6])\n",
    "print(myfiles[6:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this to extract process one file.  Used mostly for testing purposes.\n",
    "#html=webscraper_detail(myfiles[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This function will extract all page files and call process_links()\n",
    "def webscraper_detail(files):\n",
    "    #print(files,'\\n')\n",
    "    for file in files:\n",
    "        file_in  =file +'.csv'\n",
    "        file_out =file +'_detail.csv'\n",
    "        \n",
    "        df = pd.read_csv(file_in, encoding='latin1')\n",
    "        df2=df[['links','location']]\n",
    "\n",
    "        #html= process_links(df2[:2], file_in, file_out)  ## raj testing\n",
    "        html=process_links(df2, file_in, file_out)                \n",
    "    return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This function will extract details and output to new csv file.\n",
    "def process_links(df, file_in, file_out='test_output.csv'):\n",
    "    #chromedriver = \"chromedriver.exe\"\n",
    "    chromedriver = r\"C:\\Users\\roni_\\Documents\\1Roni\\GA\\Install\\chromedriver\\chromedriver.exe\"\n",
    "    os.environ[\"webdriver.chrome.driver\"] = chromedriver\n",
    "\n",
    "    driver = webdriver.Chrome(executable_path=chromedriver)\n",
    "\n",
    "    ref=[]\n",
    "    job_titles=[]\n",
    "    companys=[]\n",
    "    employment_types=[]\n",
    "    senioritys=[]\n",
    "    industrys=[]\n",
    "    salary=[]\n",
    "    salary_terms=[]\n",
    "    job_descriptions=[]\n",
    "    job_requirements=[]\n",
    "    weblinks=[]\n",
    "    location=[]\n",
    "    gov_support=[]\n",
    "\n",
    "    rec_count=0\n",
    "    range_count=0\n",
    "\n",
    "    print('Processing',file_in,':')\n",
    "    \n",
    "    for link, loc in df.itertuples(index=False):\n",
    "        ref.append(link)\n",
    "        driver.get(link)\n",
    "        sleep(8)\n",
    "        # Grab the page source.div class=\"bg-white pa4 mb3\"\n",
    "        html = driver.page_source\n",
    "        # Beautiful Soup it!\n",
    "        html = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "        # Location\n",
    "        location.append(loc)\n",
    "        # gov_support\n",
    "        gov_supp='NA'\n",
    "        for entry in html.find_all('section', {'class':'jobSchemes w-100 mb3 dib tl pa4 bg-washed-yellow bt bw2 b--light-yellow'}):\n",
    "            gov_supp=entry.find('p',{'class':'f5 fw7 pa0 mh0 mt0 mb4 brand-sec'}).text\n",
    "\n",
    "        if gov_supp == 'NA':\n",
    "            ## NO Govt Support Section Found\n",
    "            gov_support.append(np.nan)\n",
    "        else:\n",
    "            gov_support.append(gov_supp)\n",
    "\n",
    "        # search for the rest of the fields\n",
    "        for entry in html.find_all('div', {'class':'bg-white pa4 mb3'}):\n",
    "\n",
    "            #Grab the Jobtitle.\n",
    "            job_title = entry.find('h1', {'class': 'f3 fw6 mv0 pv0 mb1 dark-pink w-100 dib'}).text\n",
    "            job_titles.append(job_title)\n",
    "            #print(job_title)\n",
    "\n",
    "            # Grab the Company.\n",
    "            try:\n",
    "                company = entry.find('p', {'class': 'f6 fw6 mv0 black-80 mr2 di ttu'}).text\n",
    "                companys.append(company)\n",
    "            except:\n",
    "                companys.append(np.nan)\n",
    "\n",
    "            #seniorty\n",
    "            try:\n",
    "                seniority=entry.find('p',{'class': 'black-60 f6 fw4 mv1 dib pr3 mr1 icon-bw-seniority'}).text\n",
    "                senioritys.append(seniority)\n",
    "            except:\n",
    "                senioritys.append(np.nan)\n",
    "\n",
    "            #employment_type\n",
    "            try:\n",
    "                employment_type=entry.find('p',{'class':'black-60 f6 fw4 mv1 dib pr3 mr1 icon-bw-employment-type'}).text\n",
    "                employment_types.append(employment_type)\n",
    "            except:\n",
    "                employment_types.append(np.nan)\n",
    "\n",
    "            #industry:\n",
    "            try:\n",
    "                industry=entry.find('p',{'class':'black-60 f6 fw4 mv1 dib pr3 mr1 icon-bw-category'}).text\n",
    "                industrys.append(industry)\n",
    "            except:\n",
    "                industrys.append(np.nan)\n",
    "\n",
    "            #salary:\n",
    "            for row in entry.findAll(\"span\", class_ = re.compile(\"salary_range\")):\n",
    "                salary.append(row.text)\n",
    "\n",
    "            #monthly/yearly\n",
    "            try:\n",
    "                salary_term=entry.find('span', {'class':'salary_type dib f5 fw4 black-60 pr1 i pb'}).text\n",
    "                salary_terms.append(salary_term)\n",
    "            except:\n",
    "                salary_terms.append(np.nan)\n",
    "\n",
    "            #job description\n",
    "            try:\n",
    "                job_description=entry.find('div', {'id': 'description-content'}).text\n",
    "                job_descriptions.append(job_description)\n",
    "            except:\n",
    "                job_descriptions.append(np.nan)\n",
    "\n",
    "            #requirements\n",
    "            try:\n",
    "                job_requirement=entry.find('div', {'id': 'requirements'}).text\n",
    "                job_requirements.append(job_requirement)\n",
    "            except:\n",
    "                job_requirements.append(np.nan)\n",
    "\n",
    "            rec_count = rec_count + 1\n",
    "            range_count = range_count + 1\n",
    "\n",
    "            if range_count == 10:\n",
    "                print ('   processing', rec_count, 'records')\n",
    "                range_count = 0\n",
    "\n",
    "            #print('   ', rec_count,':',gov_supp,job_title)\n",
    "\n",
    "    # First, create an empty DataFrame.\n",
    "    import pandas as pd\n",
    "\n",
    "    column_headers=[\"job_titles\", \"companys\", \"employment_type\", \"seniority\",\n",
    "             \"industrys\", \"salary\", \"salary_terms\", \"job_descriptions\",\n",
    "             \"job_requirements\", \"links\", \"location\", \"gov_support\"]\n",
    "\n",
    "    jobs = pd.DataFrame(columns= column_headers)\n",
    "\n",
    "    jobs[\"job_titles\"]=job_titles\n",
    "    jobs[\"companys\"]=companys\n",
    "    jobs[\"employment_type\"]=employment_types\n",
    "    jobs[\"seniority\"]= senioritys\n",
    "    jobs[\"gov_support\"]=gov_support\n",
    "    jobs[\"industrys\"]=industrys\n",
    "    jobs[\"salary\"]=salary\n",
    "    jobs[\"salary_terms\"]=salary_terms\n",
    "    jobs[\"job_descriptions\"]=job_descriptions\n",
    "    jobs[\"job_requirements\"]=job_requirements\n",
    "    jobs[\"links\"]=ref\n",
    "    jobs[\"location\"]=location\n",
    "\n",
    "    jobs.to_csv(file_out)\n",
    "    driver.close()\n",
    "    print('==========================')\n",
    "    print('Records Written({}):'.format(file_out),rec_count)\n",
    "    print('==========================')\n",
    "    print()\n",
    "    \n",
    "    return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Call function to webscrape from detail files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['outfile_03', 'outfile_04', 'outfile_05'] \n",
      "\n",
      "Processing outfile_03.csv :\n",
      "   processing 10 records\n",
      "   processing 20 records\n",
      "   processing 30 records\n",
      "   processing 40 records\n",
      "   processing 50 records\n",
      "   processing 60 records\n",
      "   processing 70 records\n",
      "   processing 80 records\n",
      "   processing 90 records\n",
      "   processing 100 records\n",
      "==========================\n",
      "Records Written(outfile_03_detail.csv): 104\n",
      "==========================\n",
      "\n",
      "Processing outfile_04.csv :\n",
      "   processing 10 records\n",
      "   processing 20 records\n",
      "   processing 30 records\n",
      "   processing 40 records\n",
      "   processing 50 records\n",
      "   processing 60 records\n",
      "   processing 70 records\n",
      "   processing 80 records\n",
      "   processing 90 records\n",
      "   processing 100 records\n",
      "   processing 110 records\n",
      "   processing 120 records\n",
      "   processing 130 records\n",
      "   processing 140 records\n",
      "==========================\n",
      "Records Written(outfile_04_detail.csv): 147\n",
      "==========================\n",
      "\n",
      "Processing outfile_05.csv :\n",
      "   processing 10 records\n",
      "   processing 20 records\n",
      "   processing 30 records\n",
      "   processing 40 records\n",
      "   processing 50 records\n",
      "   processing 60 records\n",
      "   processing 70 records\n",
      "   processing 80 records\n",
      "   processing 90 records\n",
      "==========================\n",
      "Records Written(outfile_05_detail.csv): 90\n",
      "==========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "html=webscraper_detail(myfiles[3:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['outfile_a01'] \n",
      "\n",
      "Processing outfile_a01.csv :\n",
      "   processing 10 records\n",
      "   processing 20 records\n",
      "   processing 30 records\n",
      "   processing 40 records\n",
      "   processing 50 records\n",
      "   processing 60 records\n",
      "   processing 70 records\n",
      "   processing 80 records\n",
      "   processing 90 records\n",
      "   processing 100 records\n",
      "   processing 110 records\n",
      "   processing 120 records\n",
      "   processing 130 records\n",
      "   processing 140 records\n",
      "   processing 150 records\n",
      "   processing 160 records\n",
      "   processing 170 records\n",
      "   processing 180 records\n",
      "   processing 190 records\n",
      "   processing 200 records\n",
      "   processing 210 records\n",
      "   processing 220 records\n",
      "   processing 230 records\n",
      "   processing 240 records\n",
      "   processing 250 records\n",
      "   processing 260 records\n",
      "==========================\n",
      "Records Written(outfile_a01_detail.csv): 266\n",
      "==========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "html=webscraper_detail([myfiles[7]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['outfile_a02'] \n",
      "\n",
      "Processing outfile_a02.csv :\n",
      "   processing 10 records\n",
      "   processing 20 records\n",
      "   processing 30 records\n",
      "   processing 40 records\n",
      "   processing 50 records\n",
      "   processing 60 records\n",
      "   processing 70 records\n",
      "   processing 80 records\n",
      "   processing 90 records\n",
      "   processing 100 records\n",
      "   processing 110 records\n",
      "   processing 120 records\n",
      "   processing 130 records\n",
      "   processing 140 records\n",
      "   processing 150 records\n",
      "   processing 160 records\n",
      "   processing 170 records\n",
      "   processing 180 records\n",
      "   processing 190 records\n",
      "   processing 200 records\n",
      "   processing 210 records\n",
      "   processing 220 records\n",
      "   processing 230 records\n",
      "   processing 240 records\n",
      "   processing 250 records\n",
      "   processing 260 records\n",
      "==========================\n",
      "Records Written(outfile_a02_detail.csv): 260\n",
      "==========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "html=webscraper_detail([myfiles[8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['outfile_a03'] \n",
      "\n",
      "Processing outfile_a03.csv :\n",
      "   processing 10 records\n",
      "   processing 20 records\n",
      "   processing 30 records\n",
      "   processing 40 records\n",
      "   processing 50 records\n",
      "   processing 60 records\n",
      "   processing 70 records\n",
      "   processing 80 records\n",
      "   processing 90 records\n",
      "   processing 100 records\n",
      "==========================\n",
      "Records Written(outfile_a03_detail.csv): 100\n",
      "==========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "html=webscraper_detail([myfiles[9]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['outfile_a04'] \n",
      "\n",
      "Processing outfile_a04.csv :\n",
      "   processing 10 records\n",
      "==========================\n",
      "Records Written(outfile_a04_detail.csv): 19\n",
      "==========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "html=webscraper_detail([myfiles[10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
