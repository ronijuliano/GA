{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='top'></a>    \n",
    "<div style=\"width:900px;background:#fdf0db;border:1px solid black;text-align:left;padding:8px;\">\n",
    "    <p>\n",
    "        <span style=\"font-size:14pt\">\n",
    "            <b>Scraping from www.mycareersfuture.sg (Search Page Results)</b>\n",
    "        </span>\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "<div style=\"width:900px;background:#f2f2f2;border:1px solid black;text-align:left;padding:8px;\">\n",
    "\n",
    "This notebook file will extract detail information from Search Page Results.\n",
    "The URL's and The last page of the results are stored in list variables and will be fed to the user defined function webscraper().\n",
    "\n",
    "\n",
    "In order to run this project please execute the following:\n",
    "1. Run 01-webscrape-pages.ipynb\n",
    "2. Run 02-webscrape-details.ipynb\n",
    "3. Move all output files created by these two notebooks to data folder.\n",
    "4. Run 03-data-val-and-cleaning.ipynb\n",
    "   - This notebook will create a final csv file in data folder.\n",
    "5. Run 04-data-modelling.ipynb\n",
    "   - This notebook will read the final csv file to perform analysis.\n",
    "   \n",
    "Please note that All output files produced by the first two notebooks (01-webscrape-pages.ipynb & 02-webscrape-details.ipynb) will be written to the current directory.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from selenium import webdriver\n",
    "# Import sleep:\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function webscraper() \n",
    ">Used to request search page and extract values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function to scan website \n",
    "def webscraper(url,pages,outfile='outfile.csv'):\n",
    "    job_titles=[]\n",
    "    companys=[]    \n",
    "    locations=[]\n",
    "    employment_types=[]\n",
    "    gov_support=[]\n",
    "    industrys=[]\n",
    "    salary_units=[]\n",
    "    salary=[]\n",
    "    links=[]\n",
    "    rec_count=0\n",
    "\n",
    "    #url='https://www.mycareersfuture.sg/search?search=data%20science&sortBy=relevancy&page={}'\n",
    "    #driver = webdriver.Chrome(executable_path=\"./chromedriver\")\n",
    "\n",
    "    chromedriver = r\"C:\\Users\\roni_\\Documents\\1Roni\\GA\\Install\\chromedriver\\chromedriver.exe\"\n",
    "    driver = webdriver.Chrome(executable_path=chromedriver)\n",
    "\n",
    "    for page in range(0,pages+1):\n",
    "        \n",
    "        print('Processing Page {}'.format(page),'...')  \n",
    "        # make request for that page\n",
    "        driver.get(url.format(page))\n",
    "        sleep(10)               \n",
    "        #print(url)\n",
    "\n",
    "        html=driver.page_source\n",
    "        # turn into a BeautifulSoup object\n",
    "        html=BeautifulSoup(html, 'lxml')\n",
    "        \n",
    "        for entry in html.find_all('a', {'class':'bg-white mb3 w-100 dib v-top pa3 no-underline flex-ns flex-wrap JobCard__card___22xP3'}):\n",
    "            # Grab the Jobtitle.\n",
    "            try:\n",
    "                job_title = entry.find('h1', {'class': 'f4-5 fw6 mv0 brand-sec dib mr2 JobCard__jobtitle___3HqOw'}).text\n",
    "                job_titles.append(job_title)\n",
    "                #print(job_title)\n",
    "                rec_count=rec_count+1\n",
    "            except:\n",
    "                break \n",
    "        \n",
    "            # Grab the Company.\n",
    "            try:\n",
    "                company = entry.find('p', {'class': 'f6 fw6 mv0 black-80 mr2 di ttu'}).text\n",
    "                companys.append(company)\n",
    "            except:\n",
    "                companys.append('NA')\n",
    "                \n",
    "            #print(company)\n",
    "        \n",
    "            #location\n",
    "            try:\n",
    "                location=entry.find('p',{'class': 'black-80 f6 fw4 mt0 mb1 dib pr3 icon-bw-location'}).text\n",
    "                locations.append(location)\n",
    "            except:\n",
    "                locations.append('NA')\n",
    "            #print(location)\n",
    "        \n",
    "            #employment_type\n",
    "            try:\n",
    "                employment_type=entry.find('p',{'class':'black-80 f6 fw4 mt0 mb1 dib pr3 icon-bw-seniority'}).text\n",
    "                employment_types.append(employment_type)\n",
    "            except:\n",
    "                employment_types.append('NA')\n",
    "        \n",
    "            # gov_support\n",
    "            try:\n",
    "                #gov_supp=entry.find('p',{'class':'green mv0 pr3 f5-5 fw4 mr2 db pb2 JobCard__greentick___3e2_Y'}).text\n",
    "                gov_supp=entry.find('p',{'class':'green mv0 pr3 f6 fw4 mr2 db pb2 JobCard__greentick___3e2_Y'}).text\n",
    "                gov_support.append(gov_supp)\n",
    "            except:\n",
    "                gov_supp='NA'   ## roni\n",
    "                gov_support.append('NA')\n",
    "            \n",
    "            #industry:\n",
    "            try:\n",
    "                industry=entry.find('p',{'class':'black-80 f6 fw4 mt0 mb1 dib pr3 icon-bw-category'}).text\n",
    "                industrys.append(industry)\n",
    "            except:\n",
    "                industrys.append('NA')\n",
    "                \n",
    "            #salary_unit:\n",
    "            try:\n",
    "                salary_unit=entry.find('span',{'class':'salary_type black-60 mv0 fw6 f5-5 lh-solid db pb2 dib i'}).text\n",
    "                salary_units.append(salary_unit)\n",
    "            except:\n",
    "                salary_units.append('NA')\n",
    "                \n",
    "            #salary:\n",
    "            for row in entry.findAll(\"span\", class_ = re.compile(\"salary_range\")):\n",
    "                salary.append(row.text)\n",
    "            \n",
    "            #links\n",
    "            link=entry.get('href')\n",
    "            link='https://www.mycareersfuture.sg'+str(link)\n",
    "            links.append(link)\n",
    "            \n",
    "            #raj test\n",
    "            print(gov_supp,job_title)\n",
    "\n",
    "    #writefile(outfile, job_titles,companys,location,employment_type,gov_support,industrys,salary,links)\n",
    "    writefile(outfile, job_titles, companys, locations, employment_types, gov_support, industrys, salary, salary_units, links)\n",
    "    driver.close()\n",
    "    print('   {} Records written:'.format(outfile),rec_count)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function writefile() \n",
    ">Used to write details we have scraped to an output CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create an empty DataFrame.\n",
    "def writefile(filename, \n",
    "              job_titles,\n",
    "              companys,\n",
    "              locations,\n",
    "              employment_types,\n",
    "              gov_support,\n",
    "              industrys,\n",
    "              salary,\n",
    "              salary_units,\n",
    "              links):\n",
    "    \n",
    "    # First, create an empty DataFrame.\n",
    "    import pandas as pd\n",
    "    outfile=''\n",
    "    outfile = pd.DataFrame \\\n",
    "       (columns=[\"job_titles\",\"companys\", \"location\", \n",
    "                 \"employment_type\", \"gov_support\", \"salary\", \n",
    "                 \"salary_unit\", \"links\"])\n",
    "\n",
    "    outfile['job_titles']=job_titles\n",
    "    outfile['companys']=companys\n",
    "    outfile['location']=locations\n",
    "    outfile['employment_type']=employment_types\n",
    "    outfile['gov_support']=gov_support\n",
    "    outfile['industrys']=industrys\n",
    "    outfile['salary']=salary\n",
    "    outfile['salary_unit']=salary_units\n",
    "    outfile['links']=links\n",
    "\n",
    "    outfile.to_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Search strings with not so large number of pages\n",
    "mylinks=['https://www.mycareersfuture.sg/search?search=data%20science&sortBy=relevancy&page={}',\n",
    "         'https://www.mycareersfuture.sg/search?search=data%20engineer&sortBy=relevancy&page={}',\n",
    "         'https://www.mycareersfuture.sg/search?search=data%20scientist&sortBy=relevancy&page={}',\n",
    "         'https://www.mycareersfuture.sg/search?search=data%20analyst&sortBy=relevancy&page={}',\n",
    "         'https://www.mycareersfuture.sg/search?search=data%20management&sortBy=relevancy&page={}',\n",
    "         'https://www.mycareersfuture.sg/search?search=deep%20learning&sortBy=relevancy&page={}']\n",
    "\n",
    "linkpages=[6, 7, 6, 6, 7, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Search strings with large number of pages\n",
    "mylinks2=['https://www.mycareersfuture.sg/search?search=python%09%09%09&sortBy=relevancy&page={}',\n",
    "         'https://www.mycareersfuture.sg/search?search=big%20data&sortBy=relevancy&page={}',\n",
    "         'https://www.mycareersfuture.sg/search?search=machine%20learning%20&sortBy=relevancy&page={}',\n",
    "         'https://www.mycareersfuture.sg/search?search=data%20analyst&sortBy=relevancy&page={}',\n",
    "         'https://www.mycareersfuture.sg/search?search=data%20consultant&sortBy=relevancy&page={}']\n",
    "\n",
    "linkpages2=[11, 15, 12, 4, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "Processing Page 0 ...\n",
      "Processing Page 1 ...\n",
      "Processing Page 2 ...\n",
      "Processing Page 3 ...\n",
      "Processing Page 4 ...\n",
      "Processing Page 5 ...\n",
      "Processing Page 6 ...\n",
      "   outfile_00.csv Records written: 140\n",
      "-------------------------------\n",
      "Processing Page 0 ...\n",
      "Processing Page 1 ...\n",
      "Processing Page 2 ...\n",
      "Processing Page 3 ...\n",
      "Processing Page 4 ...\n",
      "Processing Page 5 ...\n",
      "Processing Page 6 ...\n",
      "Processing Page 7 ...\n",
      "   outfile_01.csv Records written: 137\n",
      "-------------------------------\n",
      "Processing Page 0 ...\n",
      "Processing Page 1 ...\n",
      "Processing Page 2 ...\n",
      "Processing Page 3 ...\n",
      "Processing Page 4 ...\n",
      "Processing Page 5 ...\n",
      "Processing Page 6 ...\n",
      "   outfile_02.csv Records written: 116\n",
      "-------------------------------\n",
      "Processing Page 0 ...\n",
      "Processing Page 1 ...\n",
      "Processing Page 2 ...\n",
      "Processing Page 3 ...\n",
      "Processing Page 4 ...\n",
      "Processing Page 5 ...\n",
      "Processing Page 6 ...\n",
      "   outfile_03.csv Records written: 104\n",
      "-------------------------------\n",
      "Processing Page 0 ...\n",
      "Processing Page 1 ...\n",
      "Processing Page 2 ...\n",
      "Processing Page 3 ...\n",
      "Processing Page 4 ...\n",
      "Processing Page 5 ...\n",
      "Processing Page 6 ...\n",
      "Processing Page 7 ...\n",
      "   outfile_04.csv Records written: 147\n",
      "-------------------------------\n",
      "Processing Page 0 ...\n",
      "Processing Page 1 ...\n",
      "Processing Page 2 ...\n",
      "Processing Page 3 ...\n",
      "Processing Page 4 ...\n",
      "   outfile_05.csv Records written: 90\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(mylinks)):\n",
    "    file='outfile_a0' + str(i) + '.csv'\n",
    "    print('-------------------------------')\n",
    "    webscraper(mylinks[i],linkpages[i],file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "Processing Page 0 ...\n",
      "Processing Page 1 ...\n",
      "Processing Page 2 ...\n",
      "Processing Page 3 ...\n",
      "Processing Page 4 ...\n",
      "Processing Page 5 ...\n",
      "Processing Page 6 ...\n",
      "Processing Page 7 ...\n",
      "Processing Page 8 ...\n",
      "Processing Page 9 ...\n",
      "Processing Page 10 ...\n",
      "Processing Page 11 ...\n",
      "   outfile_a00.csv Records written: 236\n",
      "-------------------------------\n",
      "Processing Page 0 ...\n",
      "Processing Page 1 ...\n",
      "Processing Page 2 ...\n",
      "Processing Page 3 ...\n",
      "Processing Page 4 ...\n",
      "Processing Page 5 ...\n",
      "Processing Page 6 ...\n",
      "Processing Page 7 ...\n",
      "Processing Page 8 ...\n",
      "Processing Page 9 ...\n",
      "Processing Page 10 ...\n",
      "Processing Page 11 ...\n",
      "Processing Page 12 ...\n",
      "Processing Page 13 ...\n",
      "Processing Page 14 ...\n",
      "Processing Page 15 ...\n",
      "   outfile_a01.csv Records written: 266\n",
      "-------------------------------\n",
      "Processing Page 0 ...\n",
      "Processing Page 1 ...\n",
      "Processing Page 2 ...\n",
      "Processing Page 3 ...\n",
      "Processing Page 4 ...\n",
      "Processing Page 5 ...\n",
      "Processing Page 6 ...\n",
      "Processing Page 7 ...\n",
      "Processing Page 8 ...\n",
      "Processing Page 9 ...\n",
      "Processing Page 10 ...\n",
      "Processing Page 11 ...\n",
      "Processing Page 12 ...\n",
      "   outfile_a02.csv Records written: 260\n",
      "-------------------------------\n",
      "Processing Page 0 ...\n",
      "Processing Page 1 ...\n",
      "Processing Page 2 ...\n",
      "Processing Page 3 ...\n",
      "Processing Page 4 ...\n",
      "   outfile_a03.csv Records written: 100\n",
      "-------------------------------\n",
      "Processing Page 0 ...\n",
      "Processing Page 1 ...\n",
      "Processing Page 2 ...\n",
      "   outfile_a04.csv Records written: 19\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(mylinks2)):\n",
    "    file='outfile_a0' + str(i) + '.csv'\n",
    "    print('-------------------------------')\n",
    "    webscraper(mylinks2[i],linkpages2[i],file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Page 0 ...\n",
      "NA Senior Data Engineer (Data Science and Artificial Intelligence)\n",
      "Government support for those between jobs Applied Data Science Associate\n",
      "Government support for those between jobs Data Science Lead, Large Customer Sales - Singapore\n",
      "NA Data Science Engineer\n",
      "NA Presales Specialist - Data Science\n",
      "Government support for those between jobs Assistant Manager, Data Science\n",
      "Government support for those between jobs Senior / Data Engineer (Data Science Team)\n",
      "NA Data Science Consulting\n",
      "Government support for those between jobs Data Science Engineer\n",
      "No relevant experience required Data Science Intern\n",
      "Government support for those between jobs Data Science Analyst\n",
      "NA Senior Research Fellow (Institute of Data Science)\n",
      "NA Senior Manager / Assistant Director (Data Science), NUHS Management Decision Support Office\n",
      "NA Data Science Analyst\n",
      "Government support for those between jobs Data Science Engineer\n",
      "NA Research Associate (Data Science) - EEE / INFINITUS\n",
      "Government support for those between jobs Data Science\n",
      "Government support for those between jobs Data Science Lead\n",
      "Government support for those between jobs Principal  /  Master Data Science & Application Engineer\n",
      "Government support for those between jobs Senior  /  Principal Data Science & Application Engineer\n",
      "Processing Page 1 ...\n",
      "Government support for those between jobs Junior  /  Senior Data Science & Application Engineer\n",
      "Government support for those between jobs Vice President, Data Science\n",
      "Government support for those between jobs Analyst, Operations Intelligence Data Science Specialist, CBGO, T&O - (WD03335)\n",
      "Government support for those between jobs Associate, Operations Intelligence Data Science Specialist, CBGO, T&O (WD03335)\n",
      "NA Data Scientist\n",
      "NA Data Scientist\n",
      "NA Marketing Science Partner\n",
      "NA Vice President, Enterprise Data Warehouse\n",
      "Government support for those between jobs Senior Associate, Junior Data Scientist, DBS Transformation Group, T&O (WD03390)\n",
      "Government support for those between jobs Associate, Junior Data Scientist, DBS Transformation Group, T&O (WD03390)\n",
      "Government support for those between jobs Senior Associate, Junior Data Scientist, DBS Transformation Group, T&O (WD03574)\n",
      "Government support for those between jobs Associate, Junior Data Scientist, DBS Transformation Group, T&O (WD03574)\n",
      "Government support for those between jobs Data Scientist\n",
      "NA Senior / Principal Infocomm Specialist (Data & Analytics Architecture)\n",
      "Government support for those between jobs Associate, Data Engineer\n",
      "NA Lecturer - IT Applications Development\n",
      "Government support for those between jobs Data Scientist\n",
      "Government support for those between jobs Assistant Professor\n",
      "No relevant experience required Assistant Director /  Senior Executive (Curriculum Planning and Analytics), Centre for Learning Systems\n",
      "Government support for those between jobs Data Engineer\n",
      "Processing Page 2 ...\n",
      "Government support for those between jobs Data Visualisation Developer\n",
      "NA Software Engineer in Machine Learning\n",
      "NA Data Scientist (Human Capital)\n",
      "Government support for those between jobs Staff Software Engineer\n",
      "Government support for those between jobs Data Scientist\n",
      "NA Office of Planning - Manager (Data Analyst / Scientist)\n",
      "Government support for those between jobs Business Intelligence Analyst - Machine Engineering\n",
      "Government support for those between jobs SENIOR DATA ANALYST\n",
      "NA Lecturer - Electronics (Internet of Things (IoT) & Communication)\n",
      "Government support for those between jobs Data Analyst, APAC\n",
      "Government support for those between jobs Data Scientist\n",
      "NA Business Analyst, Customer and Digital Analytics\n",
      "NA PRODUCT MANAGER (DEX)\n",
      "Government support for those between jobs Data scientist\n",
      "Government support for those between jobs Senior Data Scientist\n",
      "Government support for those between jobs Data Scientist\n",
      "NA data scientist\n",
      "Government support for those between jobs Senior Data Analyst\n",
      "Government support for those between jobs Full Stack Developer\n",
      "NA Data Scientist\n",
      "Processing Page 3 ...\n",
      "NA Data Scientist\n",
      "Government support for those between jobs Data Scientist\n",
      "Government support for those between jobs Junior Data Scientist\n",
      "Government support for those between jobs DevOps Engineer\n",
      "Government support for those between jobs Senior Research Fellow\n",
      "Government support for those between jobs Research Fellow\n",
      "NA Backend Developer\n",
      "NA Data Scientist\n",
      "Government support for those between jobs Data Scientist Analyst\n",
      "NA People Analytics Lead, Senior Manager\n",
      "No relevant experience required Manager /  Senior Manager, Compliance Strategy & Analytics, Work Pass Division (1 year contract)\n",
      "Government support for those between jobs Senior Java Developer\n",
      "Government support for those between jobs Data Scientist\n",
      "Government support for those between jobs SVP, Program Manager (Data Analytic), Institutional Banking Group Technology, T&O (WD03269)\n",
      "NA Associate Director, Enterprise Analytics\n",
      "Government support for those between jobs Senior Backend Engineer\n",
      "Government support for those between jobs Field Operation Engineer - Big Data Engineer\n",
      "Government support for those between jobs Data Scientist\n",
      "Government support for those between jobs Marketing Analytics Manager\n",
      "NA Data Visualisation Designer\n",
      "Processing Page 4 ...\n",
      "Government support for those between jobs Data Scientist\n",
      "Government support for those between jobs Senior Data Analyst\n",
      "NA Senior Java Full Stack Developer\n",
      "NA Senior Data Scientist\n",
      "Government support for those between jobs Datawarehouse Engineer\n",
      "NA Data Engineer\n",
      "Government support for those between jobs Product Analyst\n",
      "Government support for those between jobs Product Analyst\n",
      "Government support for those between jobs Senior Data Scientist\n",
      "Government support for those between jobs Senior Hadoop Engineer\n",
      "NA Head Of Data Analytics\n",
      "NA Data Scientist\n",
      "Government support for those between jobs Associate General Manager- Digital and Analytics\n",
      "Government support for those between jobs Data Analyst\n",
      "Government support for those between jobs Junior Data Scientist\n",
      "Government support for those between jobs Senior Machine Learning Engineer\n",
      "NA Research Fellow (1-year contract),  Living Analytics Research Centre\n",
      "NA Research Fellow (1-year contract),  Living Analytics Research Centre\n",
      "NA Research Fellow (1-year contract),  Living Analytics Research Centre\n",
      "Government support for those between jobs Data Engineer\n",
      "   outfile_test.csv Records written: 100\n"
     ]
    }
   ],
   "source": [
    "## You Use this if you want to run one at a time..\n",
    "\n",
    "#i=2\n",
    "#webscraper(mylinks[i],linkpages[i],'outfile_test.csv')\n",
    "\n",
    "#webscraper(mylinks[0],4,'outfile_test.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
